#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# NOTE: This file is autogenerated and should not be edited by hand.
# Last updated on: 2024-01-22

- default_service: sdks:java:io:expansion-service:shadowJar
  description: 'Outputs a PCollection of Beam Rows, each containing a single INT64
    number called "value". The count is produced from the given "start"value and either
    up to the given "end" or until 2^63 - 1.

    To produce an unbounded PCollection, simply do not specify an "end" value. Unbounded
    sequences can specify a "rate" for output elements.

    In all cases, the sequence of numbers is generated in parallel, so there is no
    inherent ordering between the generated values'
  destinations:
    python: apache_beam/io/generate_sequence
  fields:
    end:
      description: The maximum number to generate (exclusive). Will be an unbounded
        sequence if left unspecified.
      nullable: true
      type: numpy.int64
    rate:
      description: Specifies the rate to generate a given number of elements per a
        given number of seconds. Applicable only to unbounded sequences.
      nullable: true
      type: Row(seconds=typing.Union[numpy.int64, NoneType], elements=<class 'numpy.int64'>)
    start:
      description: The minimum number to generate (inclusive).
      nullable: false
      type: numpy.int64
  identifier: beam:schematransform:org.apache.beam:generate_sequence:v1
  name: GenerateSequence
- default_service: sdks:java:io:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/kafka
  fields:
    autoOffsetResetConfig:
      description: "What to do when there is no initial offset in Kafka or if the\
        \ current offset does not exist any more on the server. (1) earliest: automatically\
        \ reset the offset to the earliest offset. (2) latest: automatically reset\
        \ the offset to the latest offset (3) none: throw exception to the consumer\
        \ if no previous offset is found for the consumer\u2019s group"
      nullable: true
      type: str
    bootstrapServers:
      description: "A list of host/port pairs to use for establishing the initial\
        \ connection to the Kafka cluster. The client will make use of all servers\
        \ irrespective of which servers are specified here for bootstrapping\u2014\
        this list only impacts the initial hosts used to discover the full set of\
        \ servers. This list should be in the form `host1:port1,host2:port2,...`"
      nullable: false
      type: str
    confluentSchemaRegistrySubject:
      description: ''
      nullable: true
      type: str
    confluentSchemaRegistryUrl:
      description: ''
      nullable: true
      type: str
    consumerConfigUpdates:
      description: 'A list of key-value pairs that act as configuration parameters
        for Kafka consumers. Most of these configurations will not be needed, but
        if you need to customize your Kafka consumer, you may use this. See a detailed
        list: https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html'
      nullable: true
      type: Mapping[str, str]
    errorHandling:
      description: This option specifies whether and where to output unwritable rows.
      nullable: true
      type: Row(output=<class 'str'>)
    fileDescriptorPath:
      description: The path to the Protocol Buffer File Descriptor Set file. This
        file is used for schema definition and message serialization.
      nullable: true
      type: str
    format:
      description: 'The encoding format for the data stored in Kafka. Valid options
        are: RAW,AVRO,JSON,PROTO'
      nullable: true
      type: str
    messageName:
      description: The name of the Protocol Buffer message to be used for schema extraction
        and data conversion.
      nullable: true
      type: str
    schema:
      description: The schema in which the data is encoded in the Kafka topic. For
        AVRO data, this is a schema defined with AVRO schema syntax (https://avro.apache.org/docs/1.10.2/spec.html#schemas).
        For JSON data, this is a schema defined with JSON-schema syntax (https://json-schema.org/).
        If a URL to Confluent Schema Registry is provided, then this field is ignored,
        and the schema is fetched from Confluent Schema Registry.
      nullable: true
      type: str
    topic:
      description: ''
      nullable: false
      type: str
  identifier: beam:schematransform:org.apache.beam:kafka_read:v1
  name: ReadFromKafka
- default_service: sdks:java:io:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/kafka
  fields:
    bootstrapServers:
      description: "A list of host/port pairs to use for establishing the initial\
        \ connection to the Kafka cluster. The client will make use of all servers\
        \ irrespective of which servers are specified here for bootstrapping\u2014\
        this list only impacts the initial hosts used to discover the full set of\
        \ servers. | Format: host1:port1,host2:port2,..."
      nullable: false
      type: str
    fileDescriptorPath:
      description: The path to the Protocol Buffer File Descriptor Set file. This
        file is used for schema definition and message serialization.
      nullable: true
      type: str
    format:
      description: 'The encoding format for the data stored in Kafka. Valid options
        are: RAW,JSON,AVRO,PROTO'
      nullable: false
      type: str
    messageName:
      description: The name of the Protocol Buffer message to be used for schema extraction
        and data conversion.
      nullable: true
      type: str
    producerConfigUpdates:
      description: 'A list of key-value pairs that act as configuration parameters
        for Kafka producers. Most of these configurations will not be needed, but
        if you need to customize your Kafka producer, you may use this. See a detailed
        list: https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html'
      nullable: true
      type: Mapping[str, str]
    topic:
      description: ''
      nullable: false
      type: str
  identifier: beam:schematransform:org.apache.beam:kafka_write:v1
  name: WriteToKafka
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/gcp/json_write
  fields:
    path:
      description: The file path to write to.
      nullable: false
      type: str
  identifier: beam:schematransform:org.apache.beam:json_write:v1
  name: JsonWrite
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/gcp/bigquery
  fields:
    createDisposition:
      description: ''
      nullable: false
      type: str
    tableSpec:
      description: ''
      nullable: false
      type: str
    writeDisposition:
      description: ''
      nullable: false
      type: str
  identifier: beam:schematransform:org.apache.beam:bigquery_fileloads_write:v1
  name: FileLoadsToBigQuery
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/gcp/pubsub
  fields:
    attributes:
      description: Any additional pubsub attributes that should be populated as String
        fields in the ouptut rows.
      nullable: true
      type: Sequence[str]
    attributesMap:
      description: Any additional field that should be populated with the full set
        of PubSub attributes.
      nullable: true
      type: str
    clientFactory:
      description: ''
      nullable: true
      type: Row()
    clock:
      description: ''
      nullable: true
      type: Row()
    errorHandling:
      description: Specifies how to handle errors.
      nullable: true
      type: Row(output=<class 'str'>)
    format:
      description: 'The encoding format for the data stored in Pubsub. Valid options
        are: RAW,AVRO,JSON'
      nullable: false
      type: str
    idAttribute:
      description: When reading from Cloud Pub/Sub where unique record identifiers
        are provided as Pub/Sub message attributes, specifies the name of the attribute
        containing the unique identifier. The value of the attribute can be any string
        that uniquely identifies this record. Pub/Sub cannot guarantee that no duplicate
        data will be delivered on the Pub/Sub stream. If idAttribute is not provided,
        Beam cannot guarantee that no duplicate data will be delivered, and deduplication
        of the stream will be strictly best effort.
      nullable: true
      type: str
    schema:
      description: The schema in which the data is encoded in the Pubsub topic. For
        AVRO data, this is a schema defined with AVRO schema syntax (https://avro.apache.org/docs/1.10.2/spec.html#schemas).
        For JSON data, this is a schema defined with JSON-schema syntax (https://json-schema.org/).
      nullable: false
      type: str
    subscription:
      description: 'The name of the subscription to consume data. Either a topic or
        subscription must be provided. Format: projects/${PROJECT}/subscriptions/${SUBSCRIPTION}'
      nullable: true
      type: str
    timestampAttribute:
      description: Specifies the name of the attribute that contains the timestamp,
        if any. The timestamp value is expected to be represented in the attribute
        as either (1) a numerical value representing the number of milliseconds since
        the Unix epoch. For example, if using the Joda time classes, Instant.getMillis()
        returns the correct value for this attribute. or (2) a String in RFC 3339
        format. For example, 2015-10-29T23:41:41.123Z. The sub-second component of
        the timestamp is optional, and digits beyond the first three (i.e., time units
        smaller than milliseconds) will be ignored.
      nullable: true
      type: str
    topic:
      description: 'The name of the topic to consume data from. If a topic is specified,  will
        create a new subscription for that topic and start consuming from that point.
        Either a topic or a subscription must be provided. Format: projects/${PROJECT}/topics/${TOPIC}'
      nullable: true
      type: str
  identifier: beam:schematransform:org.apache.beam:pubsub_read:v1
  name: ReadFromPubSub
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/jdbc
  fields:
    autosharding:
      description: ''
      nullable: true
      type: bool
    connectionInitSql:
      description: ''
      nullable: true
      type: Sequence[str]
    connectionProperties:
      description: ''
      nullable: true
      type: str
    driverClassName:
      description: ''
      nullable: false
      type: str
    driverJars:
      description: ''
      nullable: true
      type: str
    jdbcUrl:
      description: ''
      nullable: false
      type: str
    location:
      description: ''
      nullable: true
      type: str
    password:
      description: ''
      nullable: true
      type: str
    username:
      description: ''
      nullable: true
      type: str
    writeStatement:
      description: ''
      nullable: true
      type: str
  identifier: beam:schematransform:org.apache.beam:jdbc_write:v1
  name: WriteToJdbc
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/gcp/bigtable
  fields:
    instanceId:
      description: ''
      nullable: false
      type: str
    projectId:
      description: ''
      nullable: false
      type: str
    tableId:
      description: ''
      nullable: false
      type: str
  identifier: beam:schematransform:org.apache.beam:bigtable_read:v1
  name: ReadFromBigtable
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/gcp/bigquery
  fields:
    query:
      description: The SQL query to be executed to read from the BigQuery table.
      nullable: true
      type: str
    rowRestriction:
      description: Read only rows that match this filter, which must be compatible
        with Google standard SQL. This is not supported when reading via query.
      nullable: true
      type: str
    selectedFields:
      description: 'Read only the specified fields (columns) from a BigQuery table.
        Fields may not be returned in the order specified. If no value is specified,
        then all fields are returned. Example: "col1, col2, col3"'
      nullable: true
      type: Sequence[str]
    tableSpec:
      description: 'The fully-qualified name of the BigQuery table to read from. Format:
        [${PROJECT}:]${DATASET}.${TABLE}'
      nullable: true
      type: str
  identifier: beam:schematransform:org.apache.beam:bigquery_storage_read:v1
  name: StorageReadFromBigQuery
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/gcp/pubsublite
  fields:
    attributeId:
      description: If set, will set an attribute for each Pubsub Lite message with
        the given name and a unique value. This attribute can then be used in a ReadFromPubSubLite
        PTransform to deduplicate messages.
      nullable: true
      type: str
    attributes:
      description: List of attribute keys whose values will be pulled out as Pubsub
        Lite message attributes.  For example, if the format is `JSON` and attributes
        is `["a", "b"]` then elements of the form `Row(any_field=..., a=..., b=...)`
        will result in Pubsub Lite messages whose payload has the contents of any_field
        and whose attribute will be populated with the values of `a` and `b`.
      nullable: true
      type: Sequence[str]
    errorHandling:
      description: This option specifies whether and where to output unwritable rows.
      nullable: true
      type: Row(output=<class 'str'>)
    format:
      description: 'The encoding format for the data stored in Pubsub Lite. Valid
        options are: RAW,JSON,AVRO'
      nullable: false
      type: str
    location:
      description: The region or zone where the Pubsub Lite reservation resides.
      nullable: false
      type: str
    project:
      description: The GCP project where the Pubsub Lite reservation resides. This
        can be a project number of a project ID.
      nullable: false
      type: str
    topicName:
      description: The name of the topic to publish data into. This will be concatenated
        with the project and location parameters to build a full topic path.
      nullable: false
      type: str
  identifier: beam:schematransform:org.apache.beam:pubsublite_write:v1
  name: WriteToPubSubLite
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/gcp/pubsublite
  fields:
    attributeId:
      description: The attribute on incoming Pubsub Lite messages to use as a unique
        record identifier. When specified, the value of this attribute (which can
        be any string that uniquely identifies the record) will be used for deduplication
        of messages. If not provided, we cannot guarantee that no duplicate data will
        be delivered on the Pub/Sub stream. In this case, deduplication of the stream
        will be strictly best effort.
      nullable: true
      type: str
    attributeMap:
      description: Name of a field in which to store the full set of attributes associated
        with this message.  For example, if the format is `RAW` and `attribute_map`
        is set to `"attrs"` then this read will produce elements of the form `Row(payload=...,
        attrs=...)` where `attrs` is a Map type of string to string. If both `attributes`
        and `attribute_map` are set, the overlapping attribute values will be present
        in both the flattened structure and the attribute map.
      nullable: true
      type: str
    attributes:
      description: List of attribute keys whose values will be flattened into the
        output message as additional fields.  For example, if the format is `RAW`
        and attributes is `["a", "b"]` then this read will produce elements of the
        form `Row(payload=..., a=..., b=...)`
      nullable: true
      type: Sequence[str]
    errorHandling:
      description: This option specifies whether and where to output unwritable rows.
      nullable: true
      type: Row(output=<class 'str'>)
    format:
      description: 'The encoding format for the data stored in Pubsub Lite. Valid
        options are: RAW,AVRO,JSON'
      nullable: false
      type: str
    location:
      description: The region or zone where the Pubsub Lite reservation resides.
      nullable: false
      type: str
    project:
      description: The GCP project where the Pubsub Lite reservation resides. This
        can be a project number of a project ID.
      nullable: true
      type: str
    schema:
      description: The schema in which the data is encoded in the Kafka topic. For
        AVRO data, this is a schema defined with AVRO schema syntax (https://avro.apache.org/docs/1.10.2/spec.html#schemas).
        For JSON data, this is a schema defined with JSON-schema syntax (https://json-schema.org/).
      nullable: true
      type: str
    subscriptionName:
      description: The name of the subscription to consume data. This will be concatenated
        with the project and location parameters to build a full subscription path.
      nullable: false
      type: str
  identifier: beam:schematransform:org.apache.beam:pubsublite_read:v1
  name: ReadFromPubSubLite
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: 'Writes data to BigQuery using the Storage Write API (https://cloud.google.com/bigquery/docs/write-api).


    This expects a single PCollection of Beam Rows and outputs two dead-letter queues
    (DLQ) that contain failed rows. The first DLQ has tag [FailedRows] and contains
    the failed rows. The second DLQ has tag [FailedRowsWithErrors] and contains failed
    rows and along with their respective errors.'
  destinations:
    python: apache_beam/io/gcp/bigquery
  fields:
    autoSharding:
      description: This option enables using a dynamically determined number of Storage
        Write API streams to write to BigQuery. Only applicable to unbounded data.
      nullable: true
      type: bool
    createDisposition:
      description: 'Optional field that specifies whether the job is allowed to create
        new tables. The following values are supported: CREATE_IF_NEEDED (the job
        may create the table), CREATE_NEVER (the job must fail if the table does not
        exist already).'
      nullable: true
      type: str
    errorHandling:
      description: This option specifies whether and where to output unwritable rows.
      nullable: true
      type: Row(output=<class 'str'>)
    numStreams:
      description: Specifies the number of write streams that the Storage API sink
        will use. This parameter is only applicable when writing unbounded data.
      nullable: true
      type: numpy.int32
    table:
      description: 'The bigquery table to write to. Format: [${PROJECT}:]${DATASET}.${TABLE}'
      nullable: false
      type: str
    triggeringFrequencySeconds:
      description: Determines how often to 'commit' progress into BigQuery. Default
        is every 5 seconds.
      nullable: true
      type: numpy.int64
    useAtLeastOnceSemantics:
      description: This option enables lower latency for insertions to BigQuery but
        may ocassionally duplicate data elements.
      nullable: true
      type: bool
    writeDisposition:
      description: 'Specifies the action that occurs if the destination table already
        exists. The following values are supported: WRITE_TRUNCATE (overwrites the
        table data), WRITE_APPEND (append the data to the table), WRITE_EMPTY (job
        must fail if the table is not empty).'
      nullable: true
      type: str
  identifier: beam:schematransform:org.apache.beam:bigquery_storage_write:v2
  name: StorageWriteToBigQuery
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/gcp/bigquery
  fields:
    query:
      description: ''
      nullable: true
      type: str
    queryLocation:
      description: ''
      nullable: true
      type: str
    tableSpec:
      description: ''
      nullable: true
      type: str
    useStandardSql:
      description: ''
      nullable: true
      type: bool
  identifier: beam:schematransform:org.apache.beam:bigquery_export_read:v1
  name: ExportReadFromBigQuery
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/gcp/bigtable
  fields:
    instanceId:
      description: ''
      nullable: false
      type: str
    projectId:
      description: ''
      nullable: false
      type: str
    tableId:
      description: ''
      nullable: false
      type: str
  identifier: beam:schematransform:org.apache.beam:bigtable_write:v1
  name: WriteToBigtable
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/gcp/pubsub
  fields:
    attributes:
      description: The set of fields to write as PubSub attributes instead of part
        of the payload.
      nullable: true
      type: Sequence[str]
    attributesMap:
      description: A map field to write as PubSub attributes instead of part of the
        payload.
      nullable: true
      type: str
    errorHandling:
      description: Specifies how to handle errors.
      nullable: true
      type: Row(output=<class 'str'>)
    format:
      description: 'The encoding format for the data stored in Pubsub. Valid options
        are: RAW,AVRO,JSON'
      nullable: false
      type: str
    idAttribute:
      description: If set, will set an attribute for each Cloud Pub/Sub message with
        the given name and a unique value. This attribute can then be used in a ReadFromPubSub
        PTransform to deduplicate messages.
      nullable: true
      type: str
    timestampAttribute:
      description: If set, will set an attribute for each Cloud Pub/Sub message with
        the given name and the message's publish time as the value.
      nullable: true
      type: str
    topic:
      description: 'The name of the topic to write data to. Format: projects/${PROJECT}/topics/${TOPIC}'
      nullable: false
      type: str
  identifier: beam:schematransform:org.apache.beam:pubsub_write:v1
  name: WriteToPubSub
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/jdbc
  fields:
    connectionInitSql:
      description: ''
      nullable: true
      type: Sequence[str]
    connectionProperties:
      description: ''
      nullable: true
      type: str
    driverClassName:
      description: ''
      nullable: false
      type: str
    driverJars:
      description: ''
      nullable: true
      type: str
    fetchSize:
      description: ''
      nullable: true
      type: numpy.int16
    jdbcUrl:
      description: ''
      nullable: false
      type: str
    location:
      description: ''
      nullable: true
      type: str
    outputParallelization:
      description: ''
      nullable: true
      type: bool
    password:
      description: ''
      nullable: true
      type: str
    readQuery:
      description: ''
      nullable: true
      type: str
    username:
      description: ''
      nullable: true
      type: str
  identifier: beam:schematransform:org.apache.beam:jdbc_read:v1
  name: ReadFromJdbc
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/gcp/spanner
  fields:
    databaseId:
      description: Specifies the Cloud Spanner database.
      nullable: false
      type: str
    instanceId:
      description: Specifies the Cloud Spanner instance.
      nullable: false
      type: str
    tableId:
      description: Specifies the Cloud Spanner table.
      nullable: false
      type: str
  identifier: beam:schematransform:org.apache.beam:spanner_write:v1
  name: WriteToSpanner
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/gcp/spanner
  fields:
    changeStreamName:
      description: Specifies the change stream name.
      nullable: false
      type: str
    databaseId:
      description: Specifies the Cloud Spanner database.
      nullable: false
      type: str
    endAtTimestamp:
      description: Specifies the end time of the change stream.
      nullable: true
      type: str
    instanceId:
      description: Specifies the Cloud Spanner instance.
      nullable: false
      type: str
    projectId:
      description: Specifies the Cloud Spanner project.
      nullable: false
      type: str
    startAtTimestamp:
      description: Specifies the time that the change stream should be read from.
      nullable: false
      type: str
    table:
      description: Specifies the Cloud Spanner table.
      nullable: false
      type: str
  identifier: beam:schematransform:org.apache.beam:spanner_cdc_read:v1
  name: ReadFromSpannerChangeStreams
- default_service: sdks:java:io:google-cloud-platform:expansion-service:shadowJar
  description: ''
  destinations:
    python: apache_beam/io/gcp/csv_write
  fields:
    path:
      description: The file path to write to.
      nullable: false
      type: str
  identifier: beam:schematransform:org.apache.beam:csv_write:v1
  name: CsvWrite
